Problem Statement-
Imagine you are working for a leading credit card company called ‘Cred Financials’. The company continuously monitors its customers’ credit card transactions, be it in any part of the world, to discover and dismiss fraudulent ones. The company also has a strong support team to address customer issues and queries.

With a rising number of fraud cases, the company’s major focus is to provide its customers with a delightful experience while ensuring that security is not compromised.
 
You, as a big data engineer, must architect and build a solution to cater to the following requirements:

Fraud detection solution: This is a feature to detect fraudulent transactions, wherein once a cardmember swipes his/her card for payment, the transaction should be classified as fraudulent or authentic based on a set of predefined rules. If fraud is detected, then the transaction must be declined. Please note that incorrectly classifying a transaction as fraudulent will incur huge losses to the company and also provoke negative consumer sentiment. 
Customers' information: The relevant information about the customers needs to be continuously updated on a platform from where the customer support team can retrieve relevant information in real time to resolve customer complaints and queries.

Data:
Now, let’s understand the types of data you will deal with. 
The following tables containing data come into consideration for this problem:

card_member (The cardholder data is added to/updated in this table by a third-party service) 
card_id – Card number,
member_id – 15-digit member ID of the cardholder,
member_joining_dt – Date and time of joining of a new member,
card_purchase_dt – Date when the card was purchased,
country – Country in which the card was purchased,
city – City in which the card was purchased
card_transactions (All incoming transactions(fraud/genuine) swiped at POS terminals are stored in this table. Earlier the transactions were classified as fraud or genuine in a traditional way. However, with an explosive surge in the number of transactions, a Big Data solution is needed to authenticate the incoming transactions and enter the transaction data accordingly):
card_id – Card number,
member_id – 15-digit member ID of the cardholder,
amount – Amount swiped with respect to the card_id,
postcode – Zip code at which this card was swiped (marking the location of an event),
pos_id – Merchant’s POS terminal ID, using which the card has been swiped,
transaction_dt – date and time of the transaction event,
status – Whether transaction was approved or not, with Genuine/Fraud value
member_score (The member credit score data is added to / updated in this table by a third-party service):
member_id – 15-digit member ID who has this card,
score – The score assigned to a member defining his/her credit history, generated by upstream systems
 
Since card_member and member_score tables are updated by the third-party services, they are stored in a central AWS RDS. You will be given the already classified card_transactions table data in the form of a CSV file, which you can load in your NoSQL database.

The other type of data is the real-time streaming data generated by the POS (Point of Sale) systems in JSON format. The streaming data looks like this:

Transactional payload (data) attributes sent by POS terminals’ gateway API on to the Kafka topic :

card_id – Card number,

member_id – 15-digit member ID of the cardholder,

amount – Amount swiped with respect to the card_id,

pos_id – Merchant’s POS terminal ID, using which the card has been swiped,

postcode – Zip code at which this card was swiped (marking the location of an event),

transaction_dt – date and time of the transaction event

Use appropriate ingestion methods available to bring card_member and member_score data from AWS RDS and card_transactions data from the NoSQL database into the Hadoop platform. This data is then processed by running batch jobs to fill data in the look-up table. After the initial load, there will be incremental loads that should be considered before designing the architecture of the solution.
 
Once the transactions data is imported into Hadoop, batch jobs are run on the data stored to calculate the moving average and standard deviation of the last 10 transactions for each card_id. Once the moving average and standard deviation are obtained, the UCL value for each card_id is calculated. Then the relevant data is entered in the look-up table.

Whenever a new transaction occurs, retrieve the ‘postcode’ and ‘transaction_dt’ attributes from the look-up table and compare these with the current ‘postcode’ and ‘transaction_dt’ data. Use the API to calculate the speed at which the user moved from the origin. If it is ahead of the imaginable speed, this can be a possible case of fraud. In such cases, the cardholder receives a call from the credit card company executive to validate the transaction. You also need to update the last transaction data in the lookup table with the current data after the comparison is complete. This has to be done in two steps:

Initially, at one-time load, you need to fetch the latest postcode and transaction_dt of each member and store in NoSQL database with other parameters discussed above
After initiating the real-time process following each member’s transaction, update the current received transaction’s postcode and transaction_dt as the last zip code and time in the lookup table stored in the NoSQL database if and only if the transaction is approved (satisfying all 3 rules)
Once a transaction is evaluated based on the above three parameters, the transaction, along with the status (i.e Genuine or Fraud ) of the transaction, is stored in the card_transactions table in the database.
